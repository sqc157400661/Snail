# 项目总结--社区评论项目技术方案总结

## 一、项目说明：

<img src="D:\www\Snail\Go实战系列\images\Screenshot_20210526_145037_com.ziroom.ziroomcustomer.jpg" alt="Screenshot_20210526_145037_com.ziroom.ziroomcustomer" style="zoom: 25%;" />

### **业务功能：**

- 发布评论，支持回复楼层、楼中楼（@用户）
- 读取评论；按照实际、按照热度进行排序、运营可以置顶
- 删除评论，用户删除、作者删除、运营删除
- 管理评论，置顶、精选、搜索、删除、审核等



## 二、技术架构组成：

![comment](D:\www\Snail\Go实战系列\images\comment.png)



### （1）评论服务的BFF层：

主要是做一些服务聚合和编排。

1. 账户服务-校验，进行用户身份校验，判断是否用户uid是否合法或者没被拉黑。
2. 账户服务-信息，获取用户昵称、头像、等级等用户信息。
3. Filter服务-敏感词校验，校验评论是否有敏感词，有的话进行脱敏处理，校验图片是否涉黄等。
4. Filter服务-可见性，判断评论的可见性（仅对某些用户可见）。
5. Filter服务-脱敏，用户信息的脱敏处理，如用户名是手机号的同学。

### （2）评论服务层

这里是评论的核心服务层，这里的服务专注于评论的功能（Separation of Concerns设计），如发布、读取、删除、点赞、关注等，关注评论的稳定性、可用性。可以说是和业务能力做了剥离，专注于基础能力，可以更好做迭代。

#### 读：

- Cache-Aside 模式，先读取缓存，再读取存储。
- read ahead思路，用户访问了第一页，很有可能访问第二页，所以缓存会超前加载，避免频繁 cache miss（这里注意当缓存抖动时候，可能容易引起集群 thundering herd 现象，大量的请求会触发 cache rebuild，又因为使用了预加载，容易导致服务 OOM，这个担忧我们在写的逻辑里处理了）

所以我们看到回源的逻辑里，我们使用了消息队列来进行逻辑异步化，对于当前请求只返回 mysql 中部分数据即止。

#### 写：

有些热点的评论，如单身party的活动，会出现大量的请求，可能引发缓存穿透。而且性能瓶颈往往就是会MySQL层，又因为写都是要穿透到MySQL层的。所以我们对写作了一些设计

- 使用了消息队列，把对写存储的直接冲击下放到消息队列（我们认为刚发布的评论有极短的延迟(通常小于几 ms)对用户可见是可接受的）【*按照消息反压的思路，即如果存储* *latency* 升高，消费能力就下降，自然消息容易堆积，系统始终以最大化方式消费。】
- Kafka 是存在 partition 概念的，可以认为是物理上的一个小队列，一个 topic 是由一组 partition 组成的，所以 Kafka 的吞吐模型理解为: 全局并行，局部串行的生产消费方式。对于入队的消息，可以按照 hash(comment_subject) % N(partitions) 的方式进行分发。那么某个 partition 中的 评论主题的数据一定都在一起，这样方便我们串行消费。

### （3）评论Job层

核心主要是利用消息队列进行消峰处理。

### （4）评论admin后台管理层

主要面向运营的管理平台，他们会共享服务层的存储层(MySQL、Redis)。运营体系的数据大量都是检索，我们使用canal进行同步到 ES 中，整个数据的展示和检索都是通过 ES，再通过业务主键更新业务数据层，这样运营端的查询压力就下方给了独立的 fulltext search 系统。

实现思路：

- mysql binlog 中的数据被 canal 中间件流式消费，获取到业务的原始 CRUD 操作，需要回放录入到 es 中，但是 es 中的数据最终是面向运营体系提供服务能力，需要检索的数据维度比较多，在入 es 前需要做一个异构的 joiner，把单表变宽预处理好 join 逻辑，然后倒入到 es 中。
- 一般来说，运营后台的检索条件都是组合的，使用 es 的好处是避免依赖 mysql 来做多条件组合检索，同时mysql 毕竟是 oltp 面向线上联机事务处理的。通过冗余数据的方式，使用其他引擎来实现。
- es 一般会存储检索、展示、primary key 等数据，当我们操作编辑的时候，找到记录的 primary key，最后交由 comment-admin 进行运营测的 CRUD 操作。

### （5）Dependency层：账户服务、filter服务

主要是BFF层依赖的账户服务的能力和filter服务的能力

## 三、存储层设计

### MySQL:

![comment2021_5_26_1](D:\www\Snail\Go实战系列\images\comment2021_5_26_1.png)

**subject表：**

评论对象表，主要管理评论对象的，`obj_id + obj_code` 可以定位到同一类的评论数据。

**comment表：**

评论主表，主要是控制评论的索引组织。

**comment_content表：**

评论内容表，包含评论的具体内容。分页查询场景：先查询comment表，然后查`... comment_content where comment_id in(....)`

**说明：**

- comment评论表和comment_content评论内容表是一一对应的，通过comment的id字段 => comment_content的comment_id关联。
- 表都有个主键，comment_content 没有 id，是为了减少一次二级索引查找，直接基于主键检索，不需要在回表里。
- 评论组织结构和评论内容做了分离，方便 mysql datapage 缓存更多的 row，如果和 content 耦合，会导致更大的 IO。（长远来看 content 信息可以直接使用 KV storage 存储）
- 表里有计数字段，防止做count查询。

### 缓存:

**subject_cache[string]:**

| key    | string  | xxxx |
| ------ | ------- | ---- |
| value  | string  | json |
| expire | duraton | 24h  |

**comment_cache[sorted set]**

| key        | string  |              |
| ---------- | ------- | ------------ |
| comment_id | int     |              |
| score      | double  | 排序权重策略 |
| expire     | duraton | 8h           |

**comment_content_cache[string]**

| key    | string  | comment_id |
| ------ | ------- | ---------- |
| value  | string  | json       |
| expire | duraton | 24h        |

**说明：**

- comment_subject_cache: 对应主题的缓存，value 使用 protobuf 序列化的方式存入。我们早期使用 memcache 来进行缓存，因为 redis 早期单线程模型，吞吐能力不高。
- comment_index_cache: 使用 redis sortedset 进行索引的缓存，索引即数据的组织顺序，而非数据内容。参考过百度的贴吧，他们使用自己研发的拉链存储来组织索引，我认为 mysql 作为主力存储，利用 redis 来做加速完全足够，因为 cache miss 的构建，我们前面讲过使用 kafka 的消费者中处理，预加载少量数据，通过增量加载的方式逐渐预热填充缓存，而 redis sortedset skiplist 的实现，可以做到 O(logN) + O(M) 的时间复杂度，效率很高。
  sorted set 是要增量追加的，因此必须判定 key 存在，才能 zdd。
- comment_content_cache: 对应评论内容数据，使用 protobuf 序列化的方式存入。类似的我们早期使用 memcache 进行缓存。
- 增量加载 + lazy 加载



## 四、可用性

### Singleflight代替分布式锁

对于热门的主题，如果存在缓存穿透的情况，会导致大量的同进程、跨进程的数据回源到存储层，可能会引起存储过载的情况，如何只交给同进程内，一个人去做加载存储?
使用归并回源的思路:
https://pkg.go.dev/golang.org/x/sync/singleflight
同进程只交给一个人去获取 mysql 数据，然后批量返回。同时这个 lease owner 投递一个 kafka 消息，做 index cache 的 recovery 操作。这样可以大大减少 mysql 的压力，以及大量透穿导致的密集写 kafka 的问题。
更进一步的，后续连续的请求，仍然可能会短时 cache miss，我们可以在进程内设置一个 short-lived flag，标记最近有一个人投递了 cache rebuild 的消息，直接 drop。

### 热点数据使用local cache

![1622020607570](D:\www\Snail\Go实战系列\images\1622020607570.png)

流量热点是因为突然热门的主题，被高频次的访问，因为底层的 cache 设计，一般是按照主题 key 进行一致性 hash 来进行分片，但是热点 key 一定命中某一个节点，这时候 remote cache 可能会变为瓶颈，因此做 cache 的升级 local cache 是有必要的，我们一般使用单进程自适应发现热点的思路，附加一个短时的 ttl local cache，可以在进程内吞掉大量的读请求。
在内存中使用 hashmap 统计每个 key 的访问频次，这里可以使用滑动窗口统计，即每个窗口中，维护一个 hashmap，之后统计所有未过去的 bucket，汇总所有 key 的数据。
之后使用小堆计算 TopK 的数据，自动进行热点识别。

